{
    "data": {
        "batch_size": 16,
        "num_workers": 8,
        "aug": false
    }, 
    "training": {
        "epochs": 20,
        "lr": 0.00001,
        "finetune": true,
        "plot": false,
        "save": true
    }, 
    "model": {
        "type": "attention",
        "name": "finetuned-attention",
        "reduction_ratio": 16,
        "kernel_size": 7,
        "in_channels": [128, 64, 8],
        "dropout_rate": 0.5
    }, 
    "eval": {
        "plot": false,
        "save": true
    }
}